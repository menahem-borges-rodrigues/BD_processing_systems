{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+\n",
      "|         state|monitors_number|\n",
      "+--------------+---------------+\n",
      "|    California|            170|\n",
      "|         Texas|            133|\n",
      "|     Minnesota|             94|\n",
      "|      Michigan|             92|\n",
      "|          Ohio|             91|\n",
      "|      New York|             67|\n",
      "|South Carolina|             64|\n",
      "|       Montana|             62|\n",
      "|  Pennsylvania|             61|\n",
      "|       Florida|             55|\n",
      "|       Indiana|             52|\n",
      "|      Colorado|             51|\n",
      "|North Carolina|             50|\n",
      "|      Illinois|             49|\n",
      "|    Washington|             43|\n",
      "|     Louisiana|             41|\n",
      "|       Arizona|             38|\n",
      "|        Kansas|             37|\n",
      "|       Georgia|             35|\n",
      "|      Kentucky|             34|\n",
      "+--------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "process time, real time (0.2233009999999993, 7.068371300119907)\n"
     ]
    }
   ],
   "source": [
    "# Which states have more/less monitors? (Rank states!)\n",
    "# para responder é necesário contar todas as posições diferenciadas (lat long) para cada estado\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from time import process_time,perf_counter\n",
    "t_start,r_start=process_time(),perf_counter()\n",
    "spark = SparkSession.builder.master('local[*]').appName('words').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "try :\n",
    "    lines = sc.textFile('epa_hap_daily_summary-small.csv') #load txt\n",
    "    header=lines.first()\n",
    "    l2 = lines.filter( lambda line : line !=header) #filtra apenas linhas maiores que 0\n",
    "    l3 = l2.map( lambda line : line.split(',') ) # faz o split das linhas num array\n",
    "    logRows = l3.map( lambda arr : Row( state = arr[24], mon_pos = arr[5]+arr[6])) #map rdd com colonas nomeadas \n",
    "    DFsql = spark.createDataFrame( logRows ) # transforma rdd em dataframe\n",
    "    query_1=DFsql.groupBy(col('state')).agg(countDistinct(col('mon_pos')).alias('monitors_number')).orderBy(col('monitors_number'),ascending=False)\n",
    "    query_1.show()\n",
    "    t_stop,r_stop=process_time(),perf_counter()\n",
    "    print(f'process time, real time {t_stop-t_start,r_stop-r_start}')\n",
    "    sc.stop()\n",
    "except Exception as err:\n",
    "    print(err)\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+\n",
      "|    state_name|monitors_number|\n",
      "+--------------+---------------+\n",
      "|    California|            170|\n",
      "|         Texas|            133|\n",
      "|     Minnesota|             94|\n",
      "|      Michigan|             92|\n",
      "|          Ohio|             91|\n",
      "|      New York|             67|\n",
      "|South Carolina|             64|\n",
      "|       Montana|             62|\n",
      "|  Pennsylvania|             61|\n",
      "|       Florida|             55|\n",
      "|       Indiana|             52|\n",
      "|      Colorado|             51|\n",
      "|North Carolina|             50|\n",
      "|      Illinois|             49|\n",
      "|    Washington|             43|\n",
      "|     Louisiana|             41|\n",
      "|       Arizona|             38|\n",
      "|        Kansas|             37|\n",
      "|       Georgia|             35|\n",
      "|      Kentucky|             34|\n",
      "+--------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "process time, real time (0.06992939999999948, 3.791043800069019)\n"
     ]
    }
   ],
   "source": [
    "# Which states have more/less monitors? (Rank states!)\n",
    "# para responder é necesário contar todas as posições diferenciadas (lat long) para cada estado\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from time import process_time,perf_counter\n",
    "t_start,r_start=process_time(),perf_counter()\n",
    "spark = SparkSession.builder.master('local[*]').appName('words').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "try :\n",
    "    main_data=spark.read.option('header','true').csv('epa_hap_daily_summary-small.csv')\n",
    "    query_1=main_data.groupBy(col('state_name')).agg(countDistinct(col('latitude'),col('longitude')).alias('monitors_number')).orderBy(col('monitors_number'),ascending=False)\n",
    "    query_1.show()\n",
    "    t_stop,r_stop=process_time(),perf_counter()\n",
    "    print(f'process time, real time {t_stop-t_start,r_stop-r_start}')\n",
    "    sc.stop()\n",
    "except Exception as err:\n",
    "    print(err)\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|           counties|      poluicao_media|\n",
      "+-------------------+--------------------+\n",
      "|        Sweet Grass|                 0.0|\n",
      "|             Martin|                 0.0|\n",
      "|Wrangell Petersburg|  4.5359477124183E-5|\n",
      "|   Northwest Arctic|6.333333333333333E-5|\n",
      "|     Aleutians East|1.095714285714285...|\n",
      "|              Eagle|1.162790697674418...|\n",
      "|  Matanuska-Susitna|1.275885558583106...|\n",
      "|    Kenai Peninsula|1.401041666666666...|\n",
      "|      Yukon-Koyukuk|1.538461538461538...|\n",
      "|              Lewis|1.573839662447257...|\n",
      "|         Rio Blanco|1.597222222222222...|\n",
      "|               Maui|1.712870012870013...|\n",
      "|             Hawaii|1.738858195211786...|\n",
      "|          Josephine|1.778510638297872...|\n",
      "|             Denali|1.861187845303868E-4|\n",
      "|           Okanogan|1.986374133949192E-4|\n",
      "|           Siskiyou|2.145333333333333...|\n",
      "|          Del Norte|2.147999999999999...|\n",
      "|             Powell|2.212528473804101E-4|\n",
      "|           Sublette|2.310253583241455...|\n",
      "|            Sanders|2.312809917355372...|\n",
      "|          Clackamas|2.386721991701245E-4|\n",
      "|               Taos|2.484474885844749...|\n",
      "|            Rosebud|2.500240384615386E-4|\n",
      "|         Rio Arriba|2.586697247706422...|\n",
      "|            Trinity|2.622997416020672E-4|\n",
      "|         White Pine|2.656642335766424...|\n",
      "|             Pitkin|2.821550387596899...|\n",
      "|              Teton|2.839886845827439...|\n",
      "|               Mono|2.847272727272728E-4|\n",
      "|            Clallam|2.904389721627410...|\n",
      "|           Kittitas|2.914355628058727...|\n",
      "|             Shasta|3.112704918032787...|\n",
      "|            Wallowa|3.150697674418605...|\n",
      "|             Garden|3.203977272727273E-4|\n",
      "|           Sheridan|3.259299781181619...|\n",
      "|          Roosevelt|3.271849865951744E-4|\n",
      "|             Apache|3.281208053691276E-4|\n",
      "|             Thomas|3.329333333333334E-4|\n",
      "|         Los Alamos|3.412482065997131E-4|\n",
      "|               Elko| 3.44632683658171E-4|\n",
      "|              Lemhi|3.486986301369863E-4|\n",
      "|           Campbell|3.508309455587392...|\n",
      "|           Mariposa|3.632635467980296...|\n",
      "|           Keweenaw|3.710151187904968E-4|\n",
      "|          Montezuma|3.712408759124089E-4|\n",
      "|               Coos|3.936032388663969E-4|\n",
      "|          El Dorado|4.035722411831628E-4|\n",
      "|            Alamosa|4.076510067114095E-4|\n",
      "|            Haywood|4.082536764705883E-4|\n",
      "|              Trego|4.092695214105794...|\n",
      "|         San Benito|4.291098748261474...|\n",
      "|              Burke|4.399775280898877...|\n",
      "|             Catron|4.503917910447761E-4|\n",
      "|            Ravalli|4.618085106382979...|\n",
      "|             Navajo|4.708346456692913...|\n",
      "|             Murray|4.721393034825872E-4|\n",
      "|           Billings|4.776082004555810...|\n",
      "|              Chase|  4.7978021978022E-4|\n",
      "|               Hyde|4.891370558375636E-4|\n",
      "|           Comanche|4.919457013574663E-4|\n",
      "|              Dukes|4.994545454545458E-4|\n",
      "|              Crook|              5.0E-4|\n",
      "|            Wakulla|5.017391304347829E-4|\n",
      "|           Flathead|5.286931818181819E-4|\n",
      "|            Yavapai|5.408551483420595E-4|\n",
      "|           Coconino|5.533979475484609E-4|\n",
      "|        Schoolcraft|5.594671403197158E-4|\n",
      "|           Brewster|5.631002331002332E-4|\n",
      "|             Chaves|5.646680942184157E-4|\n",
      "|              Grand|            5.775E-4|\n",
      "|            Laurens|5.806451612903226E-4|\n",
      "|              Avery|5.873626373626376E-4|\n",
      "|           Dona Ana|5.941818181818182E-4|\n",
      "|              Vilas|              6.0E-4|\n",
      "|         Jeff Davis|6.178873239436621E-4|\n",
      "|         Litchfield|6.181384248210027E-4|\n",
      "|              Taney|6.190759753593432E-4|\n",
      "|             Mohave|6.216441441441443E-4|\n",
      "|         St. Joseph|             6.25E-4|\n",
      "|              Adair|6.306818181818183E-4|\n",
      "|             Newton|6.340735294117648E-4|\n",
      "|            Mineral|6.558108108108109E-4|\n",
      "|               Winn| 6.58964143426295E-4|\n",
      "|          Culberson| 6.59703808180536E-4|\n",
      "|              Cedar|6.619471153846157E-4|\n",
      "|          Klickitat|6.641159135559924E-4|\n",
      "|           Thurston|6.664912280701753E-4|\n",
      "|             Blount|6.720827586206897E-4|\n",
      "|              Routt|6.742857142857143E-4|\n",
      "|          Deschutes|6.914285714285715E-4|\n",
      "|           Charlton|6.983966244725742E-4|\n",
      "|         Bennington| 7.01100569259962E-4|\n",
      "|             Yakima| 7.03049645390071E-4|\n",
      "|             Tucker|7.039563437926332E-4|\n",
      "|            Garrett|7.109401709401713E-4|\n",
      "|           Sandusky|7.133333333333333E-4|\n",
      "|             Cooper|7.177586206896551E-4|\n",
      "|            Socorro|7.447961630695445E-4|\n",
      "|            St John|7.534323432343237E-4|\n",
      "+-------------------+--------------------+\n",
      "only showing top 100 rows\n",
      "\n",
      "process time, real time (0.2318247000000042, 6.790377400116995)\n"
     ]
    }
   ],
   "source": [
    "#Which counties have the best/worst air quality? (Rank counties considering pollutants’level\n",
    "#linha indice 16 media de valores por dia soma(media diarias)/numero de dias \n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from time import process_time,perf_counter\n",
    "t_start,r_start=process_time(),perf_counter()\n",
    "spark = SparkSession.builder.master('local[*]').appName('words').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "try :\n",
    "    lines = sc.textFile('epa_hap_daily_summary-small.csv') #load txt\n",
    "    header=lines.first()\n",
    "    l2 = lines.filter( lambda line : line !=header) #filtra apenas linhas maiores que 0\n",
    "    l3 = l2.map( lambda line : line.split(',') ) # faz o split das linhas num array\n",
    "    logRows = l3.map( lambda arr : Row( counties = arr[25], media_pol = arr[16])) #map rdd com colunas nomeadas\n",
    "    DFsql = spark.createDataFrame( logRows ) # transforma rdd em dataframe\n",
    "    query_2=DFsql.groupBy(col('counties')).agg(avg(col(\"media_pol\")).alias('poluicao_media')).orderBy(col('poluicao_media'),asc=False)\n",
    "    query_2.show(100)\n",
    "    t_stop,r_stop=process_time(),perf_counter()\n",
    "    print(f'process time, real time {t_stop-t_start,r_stop-r_start}')\n",
    "    sc.stop()\n",
    "except Exception as err:\n",
    "    print(err)\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+\n",
      "| ano|              states|      poluicao media|\n",
      "+----+--------------------+--------------------+\n",
      "|1990|      Virgin Islands|                 0.0|\n",
      "|1990|           Wisconsin|                 0.0|\n",
      "|1990|            Oklahoma|                 0.0|\n",
      "|1990|       West Virginia|                 0.0|\n",
      "|1990|              Hawaii|1.970370370370370...|\n",
      "|1990|              Nevada|4.208000000000000...|\n",
      "|1990|              Alaska|4.420833333333333...|\n",
      "|1990|        South Dakota|            5.705E-4|\n",
      "|1990|          Washington|5.974999999999999E-4|\n",
      "|1990|             Wyoming|6.045454545454545E-4|\n",
      "|1990|                Utah|7.970588235294118E-4|\n",
      "|1990|          New Mexico|8.222222222222222E-4|\n",
      "|1990|              Oregon|8.596296296296297E-4|\n",
      "|1990|             Arizona|8.620134228187919E-4|\n",
      "|1990|               Maine|9.789285714285713E-4|\n",
      "|1990|            Colorado|0.002162374100719...|\n",
      "|1990|         Mississippi|0.002666666666666...|\n",
      "|1990|            Missouri|              0.0056|\n",
      "|1990|            Michigan|0.006559896373056996|\n",
      "|1990|         Connecticut|              0.0081|\n",
      "|1990|             Georgia|0.008366666666666666|\n",
      "|1990|         Puerto Rico|             0.01005|\n",
      "|1990|      North Carolina|              0.0143|\n",
      "|1990|             Alabama|            0.024325|\n",
      "|1990|                Iowa|              0.0332|\n",
      "|1990|        Pennsylvania|           0.1059625|\n",
      "|1990|                Ohio|            0.135716|\n",
      "|1990|            Illinois| 0.14575701219512197|\n",
      "|1990|          New Jersey|  0.2933352941176471|\n",
      "|1990|           Minnesota|            0.306488|\n",
      "|1990|          California| 0.41153099836333895|\n",
      "|1990|      South Carolina|  0.5598140350877193|\n",
      "|1990|District Of Columbia|  0.8261508196721312|\n",
      "|1990|            Virginia|  0.8451416666666666|\n",
      "|1990|           Louisiana|  0.9145945945945947|\n",
      "|1990|              Kansas|  1.1408154574132492|\n",
      "|1990|             Florida|  1.2765626315789476|\n",
      "|1990|            New York|   1.362972027972028|\n",
      "|1990|               Texas|  1.4824716546762589|\n",
      "|1990|             Montana|  2.0686790073529413|\n",
      "|1990|       Massachusetts|  3.0246823529411766|\n",
      "|1990|             Indiana|   4.098978378378379|\n",
      "|1990|           Tennessee|  170.40093066666665|\n",
      "|1991|                Ohio|                 0.0|\n",
      "|1991|            Arkansas|                 0.0|\n",
      "|1991|              Alaska|1.760000000000000...|\n",
      "|1991|        South Dakota|2.883333333333333E-4|\n",
      "|1991|             Wyoming|            3.258E-4|\n",
      "|1991|                Utah|3.378048780487805...|\n",
      "|1991|          New Mexico|3.986666666666666...|\n",
      "|1991|              Oregon|4.138095238095238E-4|\n",
      "|1991|              Nevada|4.826315789473683...|\n",
      "|1991|       West Virginia|5.466666666666667E-4|\n",
      "|1991|              Hawaii| 6.80754716981132E-4|\n",
      "|1991|          Washington|           7.1375E-4|\n",
      "|1991|             Georgia|           8.3875E-4|\n",
      "|1991|            Kentucky|8.585714285714285E-4|\n",
      "|1991|             Arizona|8.828402366863905E-4|\n",
      "|1991|      Virgin Islands|9.576470588235293E-4|\n",
      "|1991|               Maine|0.001306521739130...|\n",
      "|1991|             Vermont|0.001559090909090909|\n",
      "|1991|            Colorado| 0.00240126213592233|\n",
      "|1991|            Illinois|0.045304637681159415|\n",
      "|1991|           Wisconsin|             0.11625|\n",
      "|1991|           Minnesota| 0.15858126373626374|\n",
      "|1991|             Florida| 0.19555000000000003|\n",
      "|1991|        Pennsylvania| 0.21666666666666667|\n",
      "|1991|            Virginia|         0.236509875|\n",
      "|1991|           Tennessee|  0.2561980769230769|\n",
      "|1991|               Texas|   0.280053185840708|\n",
      "|1991|            Michigan|  0.3136863049853373|\n",
      "|1991|            Maryland| 0.31901639344262295|\n",
      "|1991|          California|  0.6735362628865981|\n",
      "|1991|District Of Columbia|  0.6772853968253968|\n",
      "|1991|             Montana|  0.7670011501597445|\n",
      "|1991|      South Carolina|  0.7982056737588653|\n",
      "|1991|            New York|  0.8852830188679243|\n",
      "|1991|              Kansas|  0.9181956521739129|\n",
      "|1991|          New Jersey|  1.0485551612903226|\n",
      "|1991|           Louisiana|  1.2000000000000002|\n",
      "|1991|             Indiana|  1.6614237288135594|\n",
      "|1992|                Ohio|                 0.0|\n",
      "|1992|              Alaska|1.026923076923077E-4|\n",
      "|1992|              Hawaii|1.894736842105263...|\n",
      "|1992|               Idaho|2.343478260869565E-4|\n",
      "|1992|          Washington|            2.696E-4|\n",
      "|1992|                Utah|2.836538461538461...|\n",
      "|1992|          New Mexico|3.153571428571428...|\n",
      "|1992|              Nevada|3.333333333333333E-4|\n",
      "|1992|             Wyoming|4.655813953488372E-4|\n",
      "|1992|            Arkansas|4.702272727272727E-4|\n",
      "|1992|             Vermont|4.969565217391304E-4|\n",
      "|1992|        South Dakota|4.973913043478262E-4|\n",
      "|1992|             Arizona|5.238461538461539E-4|\n",
      "|1992|            Virginia|5.474074074074074E-4|\n",
      "|1992|               Maine| 6.71111111111111E-4|\n",
      "|1992|             Georgia|8.029166666666666E-4|\n",
      "|1992|       West Virginia|8.235135135135136E-4|\n",
      "|1992|              Oregon|9.852380952380952E-4|\n",
      "|1992|            Kentucky|0.001314090909090909|\n",
      "+----+--------------------+--------------------+\n",
      "only showing top 100 rows\n",
      "\n",
      "process time, real time (0.16954049999999654, 6.784885799977928)\n"
     ]
    }
   ],
   "source": [
    "#Q.3) Which states have the best/worst air quality in each year? (Rank states per year considering pollutants’ level!\n",
    "# linha 12 contem data\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from time import process_time,perf_counter\n",
    "t_start,r_start=process_time(),perf_counter()\n",
    "spark = SparkSession.builder.master('local[*]').appName('words').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "try :\n",
    "    lines = sc.textFile('epa_hap_daily_summary-small.csv') #load txt\n",
    "    header=lines.first()\n",
    "    l2 = lines.filter( lambda line : line !=header) #filtra apenas linhas maiores que 0\n",
    "    l3 = l2.map( lambda line : line.split(',') ) # faz o split das linhas num array\n",
    "    logRows = l3.map( lambda arr : Row( states = arr[24],data=arr[11], media_pol = arr[16])) #map rdd com colonas nomeadas \n",
    "    logRowsDF = spark.createDataFrame( logRows ) # transforma rdd em dataframe\n",
    "    list_ipsDF= logRowsDF.groupBy(substring(col(\"data\"),0,4).alias('ano'),col('states')).agg((sum(col(\"media_pol\"))/count(col(\"media_pol\"))).alias(\"poluicao media\")).orderBy(col('ano'),col('poluicao media'))\n",
    "    # pandas cria grafico ####################### desbloquear para gerar grafico\n",
    "    '''df = list_ipsDF.toPandas()\n",
    "    lista=['Colorado','Iowa','Michigan','Indiana','California']\n",
    "    df1 = df[df['states'].isin(lista)]\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.lineplot(x='ano', y='poluicao media', hue='states', data=df1)\n",
    "    plt.yscale('log')\n",
    "    plt.legend(title='country', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.savefig('piores_estados.png')'''\n",
    "    #############################################\n",
    "    list_ipsDF.show(100)\n",
    "    t_stop,r_stop=process_time(),perf_counter()\n",
    "    print(f'process time, real time {t_stop-t_start,r_stop-r_start}')\n",
    "    sc.stop()\n",
    "except Exception as inst:\n",
    "    print(inst)\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|          state_name|          distance|\n",
      "+--------------------+------------------+\n",
      "|            Virginia| 715.4329730260437|\n",
      "|              Alaska| 603.6996422410685|\n",
      "|               Texas|  512.183989163014|\n",
      "|             Vermont|504.06323548508084|\n",
      "|            Illinois|440.85401432128594|\n",
      "|        South Dakota|365.84513743232617|\n",
      "|             Florida| 336.5449146570832|\n",
      "|          California|328.22638131553657|\n",
      "|            Michigan| 326.4116064851129|\n",
      "|              Nevada|326.28118071973904|\n",
      "|            Nebraska| 307.1411826055287|\n",
      "|              Kansas| 292.0796841296708|\n",
      "|               Idaho|289.63507327563104|\n",
      "|             Montana| 286.8383352756942|\n",
      "|            New York| 283.7273398637172|\n",
      "|             Wyoming| 283.6405863374747|\n",
      "|              Oregon| 268.8538079232644|\n",
      "|        Pennsylvania|251.41517634057166|\n",
      "|        North Dakota|  248.421930732627|\n",
      "|            Oklahoma|236.88257437298168|\n",
      "|           Tennessee|235.97614951487606|\n",
      "|            Missouri|234.32953412794035|\n",
      "|          Washington|219.98044806799825|\n",
      "|            Kentucky|219.95151680752107|\n",
      "|                Iowa| 206.5989410368908|\n",
      "|           Wisconsin| 202.8469300797848|\n",
      "|           Minnesota|  195.068270828248|\n",
      "|                Utah| 184.9134304071158|\n",
      "|             Georgia|184.34823507347102|\n",
      "|          New Mexico|183.18912128523763|\n",
      "|            Colorado|180.25974203315081|\n",
      "|      North Carolina|179.09404815356976|\n",
      "|             Arizona|178.87574055612177|\n",
      "|             Indiana| 177.7419577718975|\n",
      "|                Ohio|176.18296482931538|\n",
      "|         Mississippi| 174.2386099835142|\n",
      "|           Louisiana|173.27631230948307|\n",
      "|               Maine| 167.7714354141443|\n",
      "|             Alabama|167.57779614651392|\n",
      "|            Arkansas|157.85010472330006|\n",
      "|              Hawaii|155.73279515048372|\n",
      "|       West Virginia| 144.4936388149375|\n",
      "|      South Carolina|131.49188669764894|\n",
      "|       New Hampshire|115.62205092346309|\n",
      "|       Massachusetts| 92.35401256790219|\n",
      "|            Maryland| 89.28759445559282|\n",
      "|          New Jersey| 80.74367303780123|\n",
      "|      Virgin Islands| 73.45806087692814|\n",
      "|            Delaware| 51.57977048023351|\n",
      "|         Connecticut|  49.9897454877856|\n",
      "|         Puerto Rico| 32.73151627576544|\n",
      "|        Rhode Island|22.192520606570362|\n",
      "|District Of Columbia| 11.75712604636944|\n",
      "+--------------------+------------------+\n",
      "\n",
      "process time, real time (0.17909899999999368, 4.61877919989638)\n"
     ]
    }
   ],
   "source": [
    "#Q.4) For each state, what is the average distance (in km) of the monitors in that state to the state center? \n",
    "#For simplicity, assume that 1 degree of latitude or logitude equals to 111 km. (Monitor dispersion per state!)\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import math\n",
    "from time import process_time,perf_counter\n",
    "t_start,r_start=process_time(),perf_counter()\n",
    "spark = SparkSession.builder.master('local[*]').appName('words').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "try :\n",
    "    main_data=spark.read.option('header','true').csv('epa_hap_daily_summary-small.csv') #data principal\n",
    "    aux_data=spark.read.option('header','true').csv('usa_states.csv').select(col('Name'), col('MinLat'),col('MaxLat'),col('MinLon'),col('MaxLon')) #data auxiliar\n",
    "    state_center=aux_data.withColumn('lat_center',(col('MinLat')+col('MaxLat'))/2).withColumn('lon_center',(col('MinLon')+col('MaxLon'))/2).select(col('Name'),col('lat_center'),col('lon_center'))\n",
    "    df1=main_data.select(col('state_name'),col('latitude'),col('longitude')).distinct()\n",
    "    df_joint=df1.join(state_center, df1.state_name ==  state_center.Name,\"inner\").withColumn('distance',(((col('latitude')-col('lat_center'))*111)**2 + ((col('longitude')-col('lon_center'))*111)**2)**(1/2))\n",
    "    query=df_joint.groupBy(col('state_name')).agg(avg(col('distance')).alias('distance')).select(col('state_name'),col('distance')).orderBy(col('distance'),ascending=False)\n",
    "    query.show(500)\n",
    "    t_stop,r_stop=process_time(),perf_counter()\n",
    "    print(f'process time, real time {t_stop-t_start,r_stop-r_start}')\n",
    "    sc.stop()\n",
    "except Exception as inst:\n",
    "    print(inst)\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+-------+-------+-------+\n",
      "|                Name|sum(NE)|sum(SE)|sum(SW)|sum(NW)|\n",
      "+--------------------+-------+-------+-------+-------+\n",
      "|                Utah|      0|      3|      3|      6|\n",
      "|              Hawaii|      2|      1|      0|      2|\n",
      "|           Minnesota|     11|     50|     21|     12|\n",
      "|                Ohio|     30|     10|     36|     15|\n",
      "|              Oregon|      3|      1|     13|     15|\n",
      "|            Arkansas|      2|      1|      5|      3|\n",
      "|District Of Columbia|      0|      1|      3|      1|\n",
      "|               Texas|     34|     72|      3|     24|\n",
      "|        North Dakota|      1|      1|      3|      2|\n",
      "|        Pennsylvania|      3|     36|     18|      4|\n",
      "|         Connecticut|      5|      0|      8|      2|\n",
      "|             Vermont|      1|      1|      5|     16|\n",
      "|            Nebraska|      1|      3|      0|      2|\n",
      "|              Nevada|      2|      5|      0|      2|\n",
      "|         Puerto Rico|      4|      0|      1|      1|\n",
      "|          Washington|      6|      2|     15|     20|\n",
      "|            Illinois|     32|      1|     14|      2|\n",
      "|            Oklahoma|     18|      2|      0|      2|\n",
      "|      Virgin Islands|      0|      0|      6|      0|\n",
      "|            Delaware|      0|      0|      2|      4|\n",
      "|              Alaska|      4|      2|      3|      3|\n",
      "|          New Mexico|      1|      2|      3|     12|\n",
      "|       West Virginia|      2|      0|      5|      3|\n",
      "|            Missouri|      9|      3|      2|      4|\n",
      "|        Rhode Island|     12|      0|      0|      1|\n",
      "|             Georgia|      4|      5|      5|     21|\n",
      "|             Montana|     11|      5|     36|     10|\n",
      "|            Michigan|     14|     74|      1|      3|\n",
      "|            Virginia|      8|      7|      4|      0|\n",
      "|      North Carolina|     26|      5|      0|     19|\n",
      "|             Wyoming|      2|      2|      2|      3|\n",
      "|              Kansas|     18|      9|      8|      2|\n",
      "|          New Jersey|     18|      1|      4|      1|\n",
      "|            Maryland|     16|      0|      0|      1|\n",
      "|             Alabama|      5|      5|      7|     14|\n",
      "|             Arizona|      2|     16|     10|     10|\n",
      "|                Iowa|      6|      8|      4|      0|\n",
      "|       Massachusetts|     11|      4|      0|      4|\n",
      "|            Kentucky|     13|      2|     16|      3|\n",
      "|           Louisiana|      0|     35|      1|      5|\n",
      "|         Mississippi|      4|     12|      5|      0|\n",
      "|           Tennessee|      8|      4|      8|      9|\n",
      "|       New Hampshire|      2|     11|      4|      0|\n",
      "|             Indiana|     18|      8|      8|     18|\n",
      "|             Florida|     27|     23|      0|      5|\n",
      "|               Idaho|      0|      3|      7|      7|\n",
      "|      South Carolina|     10|     18|      3|     33|\n",
      "|        South Dakota|      0|      4|      3|      0|\n",
      "|          California|      2|     68|     16|     84|\n",
      "|            New York|      5|     43|      3|     16|\n",
      "+--------------------+-------+-------+-------+-------+\n",
      "only showing top 50 rows\n",
      "\n",
      "process time, real time (0.12577930000000492, 5.097684199921787)\n"
     ]
    }
   ],
   "source": [
    "#Q.5) How many sensors there are per quadrant (NW, NE, SE, SW) in each state? To answer this question, you should approximate each state’s area to a rectangle as\n",
    "#defined in the file “usa_satates.csv”, and divide that area in 4 quadrants (NW,NE, SE, SW). (Count monitors per sate qudrant! \n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from time import process_time,perf_counter\n",
    "t_start,r_start=process_time(),perf_counter()\n",
    "spark = SparkSession.builder.master('local[*]').appName('words').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "try :\n",
    "    main_data=spark.read.option('header','true').csv('epa_hap_daily_summary-small.csv') #data principal\n",
    "    aux_data=spark.read.option('header','true').csv('usa_states.csv').select(col('Name'), col('MinLat'),col('MaxLat'),col('MinLon'),col('MaxLon')) #data auxiliar\n",
    "    state_center=aux_data.withColumn('lat_center',(col('MinLat')+col('MaxLat'))/2).withColumn('lon_center',(col('MinLon')+col('MaxLon'))/2).select(col('Name'),col('lat_center'),col('lon_center'))\n",
    "    df1=main_data.select(col('state_name'),col('latitude'),col('longitude')).distinct()\n",
    "    df_joint=df1.join(state_center, df1.state_name ==  state_center.Name,\"inner\")\n",
    "    df_w=df_joint.withColumn('NE',when((col('latitude')>col('lat_center')) & (col('longitude')>col('lon_center')),1).otherwise(0)).withColumn('SE',when((col('latitude')<col('lat_center')) & (col('longitude')>col('lon_center')),1).otherwise(0)).withColumn('SW',when((col('latitude')<col('lat_center')) & (col('longitude')<col('lon_center')),1).otherwise(0)).withColumn('NW',when((col('latitude')>col('lat_center')) & (col('longitude')<col('lon_center')),1).otherwise(0))\n",
    "    query=df_w.select(col('Name'),col('NE'),col('SE'),col('SW'),col('NW')).groupBy(col('Name')).agg(sum(col('NE')),sum(col('SE')),sum(col('SW')),sum(col('NW')))\n",
    "    query.show(50)\n",
    "    t_stop,r_stop=process_time(),perf_counter()\n",
    "    print(f'process time, real time {t_stop-t_start,r_stop-r_start}')\n",
    "    sc.stop()\n",
    "except Exception as inst:\n",
    "    print(inst)\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
